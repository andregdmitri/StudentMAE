{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11dced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model       | Sequential | 26.0 K | train\n",
      "1 | patch_embed | Conv2d     | 147 K  | train\n",
      "2 | backbone    | Sequential | 6.0 M  | train\n",
      "3 | norm        | LayerNorm  | 384    | train\n",
      "4 | head        | Linear     | 1.9 K  | train\n",
      "---------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.850    Total estimated model params size (MB)\n",
      "176       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s]\n",
      "Training loop ran successfully ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning import Trainer\n",
    "from neural_network.vmamba import VisualMamba\n",
    "\n",
    "def test_forward():\n",
    "    batch_size = 2\n",
    "    img_size = 224\n",
    "    num_classes = 10\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = VisualMamba(img_size=img_size, num_classes=num_classes).to(device)\n",
    "    x = torch.randn(batch_size, 3, img_size, img_size, device=device)\n",
    "\n",
    "    logits = model(x)\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    assert logits.shape == (batch_size, num_classes)\n",
    "\n",
    "def test_training_step():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ö†Ô∏è Skipping training test: Mamba requires CUDA\")\n",
    "        return\n",
    "\n",
    "    batch_size = 4\n",
    "    img_size = 224\n",
    "    num_classes = 10\n",
    "\n",
    "    x = torch.randn(16, 3, img_size, img_size, device=\"cuda\")\n",
    "    y = torch.randint(0, num_classes, (16,), device=\"cuda\")\n",
    "    dataset = TensorDataset(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    model = VisualMamba(img_size=img_size, num_classes=num_classes).cuda()\n",
    "\n",
    "    trainer = Trainer(max_epochs=1, accelerator=\"gpu\", devices=1, fast_dev_run=True)\n",
    "    trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=dataloader)\n",
    "\n",
    "    print(\"Training loop ran successfully ‚úÖ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_forward()\n",
    "    test_training_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd191aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Checkpoint not found at models/retfound/RETFound_cfp_weights.pth",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# Put your local path where you stored the checkpoint\u001b[39;00m\n\u001b[32m     52\u001b[39m     ckpt = \u001b[33m\"\u001b[39m\u001b[33mmodels/retfound/RETFound_cfp_weights.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[43mtest_retfound_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     test_retfound_training_loop(ckpt)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtest_retfound_forward\u001b[39m\u001b[34m(checkpoint_path)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_retfound_forward\u001b[39m(checkpoint_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m os.path.exists(checkpoint_path), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m     num_classes = \u001b[32m5\u001b[39m\n\u001b[32m     10\u001b[39m     batch_size = \u001b[32m2\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Checkpoint not found at models/retfound/RETFound_cfp_weights.pth"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning import Trainer\n",
    "from neural_network.retfound import RETFoundClassifier  # adjust as needed\n",
    "\n",
    "def test_retfound_forward(checkpoint_path: str):\n",
    "    assert os.path.exists(checkpoint_path), f\"Checkpoint not found at {checkpoint_path}\"\n",
    "    num_classes = 5\n",
    "    batch_size = 2\n",
    "    img_size = 224\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = RETFoundClassifier(num_classes=num_classes, checkpoint_path=checkpoint_path).to(device)\n",
    "    x = torch.randn(batch_size, 3, img_size, img_size, device=device)\n",
    "    y = torch.randint(0, num_classes, (batch_size,), device=device)\n",
    "\n",
    "    logits = model(x)\n",
    "    print(\"Output shape:\", logits.shape)\n",
    "    assert logits.shape == (batch_size, num_classes), f\"Expected {(batch_size, num_classes)}, got {logits.shape}\"\n",
    "\n",
    "    loss = model.training_step((x, y), batch_idx=0)\n",
    "    print(\"Training loss:\", loss.item())\n",
    "\n",
    "    model.validation_step((x, y), batch_idx=0)\n",
    "\n",
    "def test_retfound_training_loop(checkpoint_path: str):\n",
    "    # Skip full training if no GPU (depending on your model‚Äôs GPU requirements)\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ö†Ô∏è Skipping training loop test: no CUDA available\")\n",
    "        return\n",
    "\n",
    "    num_classes = 5\n",
    "    img_size = 224\n",
    "    batch_size = 4\n",
    "\n",
    "    x = torch.randn(32, 3, img_size, img_size, device=\"cuda\")\n",
    "    y = torch.randint(0, num_classes, (32,), device=\"cuda\")\n",
    "    dataset = TensorDataset(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = RETFoundClassifier(num_classes=num_classes, checkpoint_path=checkpoint_path).cuda()\n",
    "\n",
    "    trainer = Trainer(max_epochs=1, accelerator=\"gpu\", devices=1, fast_dev_run=True)\n",
    "    trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=dataloader)\n",
    "\n",
    "    print(\"Training loop completed successfully\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Put your local path where you stored the checkpoint\n",
    "    ckpt = \"models/RETFound_cfp_weights.pth\"\n",
    "    test_retfound_forward(ckpt)\n",
    "    test_retfound_training_loop(ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea10844f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
