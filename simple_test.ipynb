{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2858024",
   "metadata": {},
   "source": [
    "# RetFound Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11dced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model       | Sequential | 26.0 K | train\n",
      "1 | patch_embed | Conv2d     | 147 K  | train\n",
      "2 | backbone    | Sequential | 6.0 M  | train\n",
      "3 | norm        | LayerNorm  | 384    | train\n",
      "4 | head        | Linear     | 1.9 K  | train\n",
      "---------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.850    Total estimated model params size (MB)\n",
      "176       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.86it/s]\n",
      "Training loop ran successfully âœ…\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning import Trainer\n",
    "from neural_network.vmamba import VisualMamba\n",
    "\n",
    "def test_forward():\n",
    "    batch_size = 2\n",
    "    img_size = 224\n",
    "    num_classes = 10\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = VisualMamba(img_size=img_size, num_classes=num_classes, mask_ratio=0.75).to(device)\n",
    "    x = torch.randn(batch_size, 3, img_size, img_size, device=device)\n",
    "\n",
    "    logits = model(x)\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    assert logits.shape == (batch_size, num_classes)\n",
    "\n",
    "def test_training_step():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"âš ï¸ Skipping training test: Mamba requires CUDA\")\n",
    "        return\n",
    "\n",
    "    batch_size = 4\n",
    "    img_size = 224\n",
    "    num_classes = 10\n",
    "\n",
    "    x = torch.randn(16, 3, img_size, img_size, device=\"cuda\")\n",
    "    y = torch.randint(0, num_classes, (16,), device=\"cuda\")\n",
    "    dataset = TensorDataset(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    model = VisualMamba(img_size=img_size, num_classes=num_classes, mask_ratio=0.75).cuda()\n",
    "\n",
    "    trainer = Trainer(max_epochs=1, accelerator=\"gpu\", devices=1, fast_dev_run=True)\n",
    "    trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=dataloader)\n",
    "\n",
    "    print(\"Training loop ran successfully âœ…\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_forward()\n",
    "    test_training_step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04120fd0",
   "metadata": {},
   "source": [
    "# Distillation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3380f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys after loading checkpoint: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']\n",
      "[Distill] epoch 1  loss=0.996899\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from distillation.dist import EmbeddingDataset, distill_embeddings\n",
    "from models.retfound import RETFoundClassifier\n",
    "from models.vmamba import VisualMamba\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "teacher = RETFoundClassifier(\n",
    "    checkpoint_path=\"checkpoints/RETFound_cfp_weights.pth\"\n",
    ").to(device)\n",
    "\n",
    "student = VisualMamba(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=24,\n",
    "    learning_rate=1e-4\n",
    ").to(device)\n",
    "\n",
    "projector = nn.Linear(192, 1024).to(device)\n",
    "\n",
    "dummy_data = torch.randn(1000, 3, 224, 224)\n",
    "embed_loader = DataLoader(EmbeddingDataset(dummy_data), batch_size=32, shuffle=True)\n",
    "\n",
    "# ---------------- phase 1 ----------------\n",
    "distill_embeddings(\n",
    "    teacher=teacher,\n",
    "    student=student,\n",
    "    projector=projector,\n",
    "    dataloader=embed_loader,\n",
    "    device=device,\n",
    "    epochs=1,\n",
    ")\n",
    "\n",
    "torch.save(student.state_dict(), \"vmamba_distilled.pth\")\n",
    "\n",
    "# ================================\n",
    "# phase 2 lightning finetuning\n",
    "# ================================\n",
    "\n",
    "# # freeze backbone â€” only train heads\n",
    "# for name, p in student.named_parameters():\n",
    "#     if not name.startswith(\"heads.\"):\n",
    "#         p.requires_grad_(False)\n",
    "\n",
    "# # your real labeled dataset\n",
    "# train_loader = DataLoader(real_dataset, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(real_val, batch_size=16)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     max_epochs=10,\n",
    "#     accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "# )\n",
    "\n",
    "# trainer.fit(student, train_loader, val_loader)\n",
    "\n",
    "# torch.save(student.state_dict(), \"vmamba_heads_trained.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4d70f",
   "metadata": {},
   "source": [
    "# Final model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23bb8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103 images for split=test\n",
      "Loading checkpoint: checkpoints/vmamba_final_head_v3_unmasked.pth\n",
      "Running predictions on IDRiD test set...\n",
      "[âœ“] Saved predictions to vmamba_idrid_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from config.constants import *\n",
    "from dataloader.idrid import IDRiDDataset\n",
    "from models.vmamba_backbone import VisualMamba\n",
    "\n",
    "\n",
    "MODEL_PATH = \"checkpoints/vmamba_final_head_v3_unmasked.pth\"\n",
    "CSV_OUT = \"vmamba_idrid_predictions.csv\"\n",
    "\n",
    "\n",
    "class VmambaFinalModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Loads the final backbone + head exactly as stored in\n",
    "    vmamba_final_head_v3_unmasked.pth\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # ----------------------------\n",
    "        # 1. Rebuild VisualMamba Backbone\n",
    "        # ----------------------------\n",
    "        self.backbone = VisualMamba(\n",
    "            img_size=IMG_SIZE,\n",
    "            patch_size=PATCH_SIZE,\n",
    "            in_chans=IN_CHANS,\n",
    "            embed_dim=VMAMBA_EMBED_DIM,\n",
    "            depth=VMAMBA_DEPTH,\n",
    "            mask_ratio=0.0,\n",
    "            use_cls_token=False,\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # 2. Rebuild Classification Head (must match training)\n",
    "        # ----------------------------\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone.embed_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(128, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "        # ----------------------------\n",
    "        # 3. Load Weights\n",
    "        # ----------------------------\n",
    "        print(f\"Loading checkpoint: {MODEL_PATH}\")\n",
    "        ckpt = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "\n",
    "        if \"state_dict\" in ckpt:\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "        else:\n",
    "            state_dict = ckpt\n",
    "\n",
    "        backbone_state = {}\n",
    "        head_state = {}\n",
    "\n",
    "        for k, v in state_dict.items():\n",
    "            # backbone.*\n",
    "            if k.startswith(\"backbone.\"):\n",
    "                backbone_state[k[len(\"backbone.\"):]] = v\n",
    "            # head.*\n",
    "            elif k.startswith(\"head.\"):\n",
    "                head_state[k[len(\"head.\"):]] = v\n",
    "\n",
    "        self.backbone.load_state_dict(backbone_state)\n",
    "        self.head.load_state_dict(head_state)\n",
    "\n",
    "        # Freeze all parameters\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            feats = self.backbone.forward_features(x)\n",
    "            logits = self.head(feats)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            return probs\n",
    "\n",
    "\n",
    "def run_prediction():\n",
    "    # ----------------------------\n",
    "    # Transform (must match training!)\n",
    "    # ----------------------------\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # ----------------------------\n",
    "    # Test dataset\n",
    "    # ----------------------------\n",
    "    test_ds = IDRiDDataset(IDRID_PATH, split=\"test\", transform=tfm)\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Load model\n",
    "    # ----------------------------\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = VmambaFinalModel().to(device)\n",
    "\n",
    "    paths = []\n",
    "    labels = []\n",
    "    preds = []\n",
    "    probs_list = []\n",
    "\n",
    "    print(\"Running predictions on IDRiD test set...\")\n",
    "\n",
    "    for img, label, path in test_loader:\n",
    "        img = img.to(device)\n",
    "\n",
    "        probs = model(img)\n",
    "        pred = torch.argmax(probs, dim=1)\n",
    "\n",
    "        paths.append(path[0])\n",
    "        labels.append(label.item())\n",
    "        preds.append(pred.item())\n",
    "        probs_list.append(probs.cpu().numpy()[0].tolist())\n",
    "\n",
    "    # ----------------------------\n",
    "    # Save to CSV\n",
    "    # ----------------------------\n",
    "    df = pd.DataFrame({\n",
    "        \"image_path\": paths,\n",
    "        \"true_label\": labels,\n",
    "        \"pred_label\": preds,\n",
    "    })\n",
    "\n",
    "    prob_cols = [f\"prob_{i}\" for i in range(NUM_CLASSES)]\n",
    "    df_probs = pd.DataFrame(probs_list, columns=prob_cols)\n",
    "\n",
    "    df = pd.concat([df, df_probs], axis=1)\n",
    "    df.to_csv(CSV_OUT, index=False)\n",
    "\n",
    "    print(f\"[âœ“] Saved predictions to {CSV_OUT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "938a5425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103 images for split=test\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'config' has no attribute 'VMAMBA_EMBED_DIM'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 208\u001b[39m\n\u001b[32m    205\u001b[39m     diagnostics(backbone, head, batch)\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 201\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;66;03m# load model pieces\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m os.path.exists(CHECKPOINT_PATH), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHECKPOINT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m backbone, head = \u001b[43mload_backbone_and_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;66;03m# get first batch\u001b[39;00m\n\u001b[32m    204\u001b[39m batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mload_backbone_and_head\u001b[39m\u001b[34m(ckpt_path)\u001b[39m\n\u001b[32m     23\u001b[39m     state = ckpt\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Build backbone\u001b[39;00m\n\u001b[32m     26\u001b[39m backbone = VisualMamba(\n\u001b[32m     27\u001b[39m     img_size=IMG_SIZE,\n\u001b[32m     28\u001b[39m     patch_size=\u001b[32m16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28m__import__\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mconfig.constants\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mPATCH_SIZE\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m16\u001b[39m,  \u001b[38;5;66;03m# patched default - replace if needed\u001b[39;00m\n\u001b[32m     29\u001b[39m     in_chans=\u001b[32m3\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     embed_dim=\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43m__import__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconfig.constants\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mVMAMBA_EMBED_DIM\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     31\u001b[39m     depth=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28m__import__\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mconfig.constants\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mVMAMBA_DEPTH\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     32\u001b[39m     mask_ratio=\u001b[32m0.0\u001b[39m,\n\u001b[32m     33\u001b[39m     use_cls_token=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     34\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Build head (must match training)\u001b[39;00m\n\u001b[32m     37\u001b[39m head = nn.Sequential(\n\u001b[32m     38\u001b[39m     nn.Linear(backbone.embed_dim, \u001b[32m512\u001b[39m),\n\u001b[32m     39\u001b[39m     nn.BatchNorm1d(\u001b[32m512\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     nn.Linear(\u001b[32m128\u001b[39m, NUM_CLASSES)\n\u001b[32m     49\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: module 'config' has no attribute 'VMAMBA_EMBED_DIM'"
     ]
    }
   ],
   "source": [
    "# diagnose_vmamba.py\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from config.constants import IDRID_PATH, IMG_SIZE, CHECKPOINT_DIR, NUM_CLASSES\n",
    "from dataloader.idrid import IDRiDDataset\n",
    "from models.vmamba_backbone import VisualMamba\n",
    "import torch.nn as nn\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, \"vmamba_final_head_v3_unmasked.pth\")\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "def load_backbone_and_head(ckpt_path):\n",
    "    \"\"\"Reconstruct VisualMamba backbone and head, load weights from checkpoint.\"\"\"\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    if \"state_dict\" in ckpt:\n",
    "        state = ckpt[\"state_dict\"]\n",
    "    else:\n",
    "        state = ckpt\n",
    "\n",
    "    # Build backbone\n",
    "    backbone = VisualMamba(\n",
    "        img_size=IMG_SIZE,\n",
    "        patch_size=16 if hasattr(__import__('config.constants'), 'PATCH_SIZE') is False else 16,  # patched default - replace if needed\n",
    "        in_chans=3,\n",
    "        embed_dim=getattr(__import__('config.constants'), 'VMAMBA_EMBED_DIM'),\n",
    "        depth=getattr(__import__('config.constants'), 'VMAMBA_DEPTH'),\n",
    "        mask_ratio=0.0,\n",
    "        use_cls_token=False,\n",
    "    )\n",
    "\n",
    "    # Build head (must match training)\n",
    "    head = nn.Sequential(\n",
    "        nn.Linear(backbone.embed_dim, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(0.3),\n",
    "\n",
    "        nn.Linear(512, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(0.2),\n",
    "\n",
    "        nn.Linear(128, NUM_CLASSES)\n",
    "    )\n",
    "\n",
    "    # split state dict into backbone / head parts\n",
    "    backbone_state = {}\n",
    "    head_state = {}\n",
    "    for k, v in state.items():\n",
    "        if k.startswith(\"backbone.\"):\n",
    "            backbone_state[k[len(\"backbone.\"):]] = v\n",
    "        elif k.startswith(\"head.\"):\n",
    "            head_state[k[len(\"head.\"):]] = v\n",
    "        # also accept keys without prefixes (older save formats)\n",
    "        elif k in backbone.state_dict().keys():\n",
    "            backbone_state[k] = v\n",
    "        elif k in head.state_dict().keys():\n",
    "            head_state[k] = v\n",
    "\n",
    "    # load weights (use strict=False to allow tiny mismatches but report)\n",
    "    missing_b, unexpected_b = backbone.load_state_dict(backbone_state, strict=False)\n",
    "    missing_h, unexpected_h = head.load_state_dict(head_state, strict=False)\n",
    "\n",
    "    print(\"Backbone load: missing keys:\", missing_b, \" unexpected:\", unexpected_b)\n",
    "    print(\"Head load:     missing keys:\", missing_h, \" unexpected:\", unexpected_h)\n",
    "\n",
    "    return backbone, head\n",
    "\n",
    "def diagnostics(backbone, head, batch):\n",
    "    \"\"\"Run a suite of diagnostics on a batch (x_cpu, y, paths).\"\"\"\n",
    "    x_cpu, y, paths = batch\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    backbone = backbone.to(device)\n",
    "    head = head.to(device)\n",
    "    backbone.eval()\n",
    "    head.eval()\n",
    "\n",
    "    print(\"\\n=== DIAGNOSTICS ===\")\n",
    "    print(\"Batch size:\", x_cpu.shape)\n",
    "\n",
    "    # move to device\n",
    "    x = x_cpu.to(device)\n",
    "\n",
    "    # 1) input stats\n",
    "    with torch.no_grad():\n",
    "        pixel_means = x.view(x.shape[0], -1).mean(dim=1).cpu().numpy()\n",
    "        pixel_stds  = x.view(x.shape[0], -1).std(dim=1).cpu().numpy()\n",
    "    print(\"\\n1) Input pixel means (per image):\", np.round(pixel_means, 6))\n",
    "    print(\"   Input pixel stds  (per image):\", np.round(pixel_stds, 6))\n",
    "\n",
    "    # 2) patch_embed output stats\n",
    "    with torch.no_grad():\n",
    "        pe = backbone.patch_embed(x)   # (B, D, H', W')\n",
    "        B, D, H_, W_ = pe.shape\n",
    "        pe_flat = pe.flatten(2).transpose(1, 2)   # (B, N, D)\n",
    "        pe_pool = pe_flat.mean(dim=1)            # (B, D)\n",
    "\n",
    "    print(f\"\\n2) Patch-embed shape: {pe.shape} -> flattened {pe_flat.shape}\")\n",
    "    m = pe_pool.mean(dim=1).cpu().numpy()\n",
    "    s = pe_pool.std(dim=1).cpu().numpy()\n",
    "    print(\"   Patch-embed pooled mean per sample:\", np.round(m, 6))\n",
    "    print(\"   Patch-embed pooled std  per sample:\", np.round(s, 6))\n",
    "    identical_pe = torch.allclose(pe_pool, pe_pool[0].unsqueeze(0).expand_as(pe_pool))\n",
    "    print(\"   Patch-embed pooled identical across batch?:\", bool(identical_pe))\n",
    "\n",
    "    # 3) full backbone forward -> pooled features\n",
    "    with torch.no_grad():\n",
    "        pooled = backbone.forward_features(x, return_pooled=True, apply_mask=False)  # (B, D)\n",
    "    print(f\"\\n3) Backbone pooled features shape: {pooled.shape}\")\n",
    "    pooled_means = pooled.mean(dim=1).cpu().numpy()\n",
    "    pooled_stds = pooled.std(dim=1).cpu().numpy()\n",
    "    print(\"   Pooled mean per sample:\", np.round(pooled_means, 6))\n",
    "    print(\"   Pooled std  per sample:\", np.round(pooled_stds, 6))\n",
    "    identical_pooled = torch.allclose(pooled, pooled[0].unsqueeze(0).expand_as(pooled))\n",
    "    print(\"   Pooled features identical across batch?:\", bool(identical_pooled))\n",
    "\n",
    "    # 4) cosine similarities between samples\n",
    "    if pooled.shape[0] >= 2:\n",
    "        f0 = pooled[0].cpu().numpy()\n",
    "        for i in range(1, min(5, pooled.shape[0])):\n",
    "            fi = pooled[i].cpu().numpy()\n",
    "            cos = np.dot(f0, fi) / (np.linalg.norm(f0) * np.linalg.norm(fi) + 1e-12)\n",
    "            print(f\"   Cosine pooled[0] vs pooled[{i}]: {cos:.6f}\")\n",
    "\n",
    "    # 5) NaN / Inf checks\n",
    "    has_nan = torch.isnan(pooled).any().item()\n",
    "    has_inf = torch.isinf(pooled).any().item()\n",
    "    print(\"\\n5) pooled has NaN?\", has_nan, \" pooled has Inf?\", has_inf)\n",
    "\n",
    "    # 6) first VimBlock activation stats\n",
    "    first_block = None\n",
    "    if hasattr(backbone, \"backbone\") and len(backbone.backbone) > 0:\n",
    "        first_block = backbone.backbone[0]\n",
    "    elif isinstance(backbone, nn.Sequential) and len(backbone) > 0:\n",
    "        first_block = backbone[0]\n",
    "\n",
    "    if first_block is not None:\n",
    "        with torch.no_grad():\n",
    "            first_out = first_block(pe_flat)  # (B, N, E)\n",
    "        print(\"\\n6) First block output shape:\", first_out.shape)\n",
    "        mean_per_sample = first_out.mean(dim=(1,2)).cpu().numpy()\n",
    "        std_per_sample = first_out.std(dim=(1,2)).cpu().numpy()\n",
    "        print(\"   mean per sample:\", np.round(mean_per_sample, 6))\n",
    "        print(\"   std  per sample:\", np.round(std_per_sample, 6))\n",
    "        identical_first = torch.allclose(first_out, first_out[0].unsqueeze(0).expand_as(first_out))\n",
    "        print(\"   First block output identical across batch?:\", bool(identical_first))\n",
    "\n",
    "        # 7) parameter sanity for first block (print a few params)\n",
    "        print(\"\\n7) First block parameter stats (sample):\")\n",
    "        found = False\n",
    "        for n, p in first_block.named_parameters():\n",
    "            found = True\n",
    "            pv = p.detach().cpu()\n",
    "            print(f\"   {n:30s} | mean={pv.mean():.6f} std={pv.std():.6f} min={pv.min():.6f} max={pv.max():.6f}\")\n",
    "            # print only a few parameters\n",
    "            break\n",
    "        if not found:\n",
    "            print(\"   (no params found in first block)\")\n",
    "\n",
    "        # 8) small gradient check: single forward/backward through first block\n",
    "        device = next(first_block.parameters()).device if any(p.requires_grad for p in first_block.parameters()) else x.device\n",
    "        x_rand = torch.randn(2, pe_flat.shape[1], pe_flat.shape[2], device=device, requires_grad=True)\n",
    "        try:\n",
    "            out = first_block(x_rand).sum()\n",
    "            out.backward()\n",
    "            grad_std = x_rand.grad.std().item() if x_rand.grad is not None else float(\"nan\")\n",
    "            print(\"\\n8) Grad check: input grad std:\", grad_std)\n",
    "        except Exception as e:\n",
    "            print(\"\\n8) Grad check failed:\", e)\n",
    "    else:\n",
    "        print(\"\\nNo first block found for inspection (structure differs).\")\n",
    "\n",
    "    # 9) patch_embed parameter stats\n",
    "    print(\"\\n9) patch_embed parameters:\")\n",
    "    pe_w = backbone.patch_embed.weight.detach().cpu()\n",
    "    print(\"   patch_embed.weight mean:\", float(pe_w.mean()), \" std:\", float(pe_w.std()))\n",
    "    if backbone.patch_embed.bias is not None:\n",
    "        print(\"   patch_embed.bias mean:\", float(backbone.patch_embed.bias.detach().cpu().mean()))\n",
    "    print(\"   patch_embed.weight any NaN?\", bool(torch.isnan(pe_w).any()))\n",
    "    if backbone.patch_embed.bias is not None:\n",
    "        print(\"   patch_embed.bias any NaN?\", bool(torch.isnan(backbone.patch_embed.bias.detach().cpu()).any()))\n",
    "\n",
    "    print(\"\\n=== DIAGNOSTICS COMPLETE ===\\n\")\n",
    "\n",
    "def main():\n",
    "    # build dataset + loader\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    ds = IDRiDDataset(root=IDRID_PATH, split=\"test\", transform=tfm)\n",
    "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # load model pieces\n",
    "    assert os.path.exists(CHECKPOINT_PATH), f\"Checkpoint not found: {CHECKPOINT_PATH}\"\n",
    "    backbone, head = load_backbone_and_head(CHECKPOINT_PATH)\n",
    "\n",
    "    # get first batch\n",
    "    batch = next(iter(loader))\n",
    "    diagnostics(backbone, head, batch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6304af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
