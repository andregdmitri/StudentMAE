{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2858024",
   "metadata": {},
   "source": [
    "# RetFound Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11dced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model       | Sequential | 26.0 K | train\n",
      "1 | patch_embed | Conv2d     | 147 K  | train\n",
      "2 | backbone    | Sequential | 6.0 M  | train\n",
      "3 | norm        | LayerNorm  | 384    | train\n",
      "4 | head        | Linear     | 1.9 K  | train\n",
      "---------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.850    Total estimated model params size (MB)\n",
      "176       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.86it/s]\n",
      "Training loop ran successfully ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning import Trainer\n",
    "from neural_network.vmamba import VisualMamba\n",
    "\n",
    "def test_forward():\n",
    "    batch_size = 2\n",
    "    img_size = 224\n",
    "    num_classes = 10\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = VisualMamba(img_size=img_size, num_classes=num_classes, mask_ratio=0.75).to(device)\n",
    "    x = torch.randn(batch_size, 3, img_size, img_size, device=device)\n",
    "\n",
    "    logits = model(x)\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    assert logits.shape == (batch_size, num_classes)\n",
    "\n",
    "def test_training_step():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ö†Ô∏è Skipping training test: Mamba requires CUDA\")\n",
    "        return\n",
    "\n",
    "    batch_size = 4\n",
    "    img_size = 224\n",
    "    num_classes = 10\n",
    "\n",
    "    x = torch.randn(16, 3, img_size, img_size, device=\"cuda\")\n",
    "    y = torch.randint(0, num_classes, (16,), device=\"cuda\")\n",
    "    dataset = TensorDataset(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    model = VisualMamba(img_size=img_size, num_classes=num_classes, mask_ratio=0.75).cuda()\n",
    "\n",
    "    trainer = Trainer(max_epochs=1, accelerator=\"gpu\", devices=1, fast_dev_run=True)\n",
    "    trainer.fit(model, train_dataloaders=dataloader, val_dataloaders=dataloader)\n",
    "\n",
    "    print(\"Training loop ran successfully ‚úÖ\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_forward()\n",
    "    test_training_step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04120fd0",
   "metadata": {},
   "source": [
    "# Distillation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3380f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys after loading checkpoint: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']\n",
      "[Distill] epoch 1  loss=0.996899\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from distillation.dist import EmbeddingDataset, distill_embeddings\n",
    "from models.retfound import RETFoundClassifier\n",
    "from models.vmamba import VisualMamba\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "teacher = RETFoundClassifier(\n",
    "    checkpoint_path=\"checkpoints/RETFound_cfp_weights.pth\"\n",
    ").to(device)\n",
    "\n",
    "student = VisualMamba(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=24,\n",
    "    learning_rate=1e-4\n",
    ").to(device)\n",
    "\n",
    "projector = nn.Linear(192, 1024).to(device)\n",
    "\n",
    "dummy_data = torch.randn(1000, 3, 224, 224)\n",
    "embed_loader = DataLoader(EmbeddingDataset(dummy_data), batch_size=32, shuffle=True)\n",
    "\n",
    "# ---------------- phase 1 ----------------\n",
    "distill_embeddings(\n",
    "    teacher=teacher,\n",
    "    student=student,\n",
    "    projector=projector,\n",
    "    dataloader=embed_loader,\n",
    "    device=device,\n",
    "    epochs=1,\n",
    ")\n",
    "\n",
    "torch.save(student.state_dict(), \"vmamba_distilled.pth\")\n",
    "\n",
    "# ================================\n",
    "# phase 2 lightning finetuning\n",
    "# ================================\n",
    "\n",
    "# # freeze backbone ‚Äî only train heads\n",
    "# for name, p in student.named_parameters():\n",
    "#     if not name.startswith(\"heads.\"):\n",
    "#         p.requires_grad_(False)\n",
    "\n",
    "# # your real labeled dataset\n",
    "# train_loader = DataLoader(real_dataset, batch_size=16, shuffle=True)\n",
    "# val_loader = DataLoader(real_val, batch_size=16)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     max_epochs=10,\n",
    "#     accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "# )\n",
    "\n",
    "# trainer.fit(student, train_loader, val_loader)\n",
    "\n",
    "# torch.save(student.state_dict(), \"vmamba_heads_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e6a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys after loading checkpoint: ['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias']\n",
      "Found 413 images for split=train\n",
      "Found 103 images for split=test\n",
      "[Distill] epoch 1  loss=0.545846\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from distillation.dist import distill_embeddings, train_head\n",
    "from models.retfound import RETFoundClassifier\n",
    "from models.vmamba import VisualMamba\n",
    "from dataloader.idrid import IDRiDModule\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# ---------------- setup ----------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "teacher = RETFoundClassifier(\n",
    "    checkpoint_path=\"checkpoints/RETFound_cfp_weights.pth\"\n",
    ").to(device)\n",
    "\n",
    "student = VisualMamba(\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_chans=3,\n",
    "    embed_dim=192,\n",
    "    depth=24,\n",
    "    learning_rate=1e-4\n",
    ").to(device)\n",
    "\n",
    "projector = nn.Linear(192, 1024).to(device)\n",
    "\n",
    "\n",
    "# ---------------- data ----------------\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dm = IDRiDModule(\n",
    "    root=\"./data/aaryapatel98/indian-diabetic-retinopathy-image-dataset/versions/1/B.%20Disease%20Grading/B. Disease Grading\",\n",
    "    transform=tfm,\n",
    "    batch_size=2\n",
    ")\n",
    "dm.setup()\n",
    "train_loader = dm.train_dataloader()\n",
    "val_loader   = dm.val_dataloader()\n",
    "\n",
    "\n",
    "# ---------------- phase 1 ----------------\n",
    "distill_embeddings(\n",
    "    teacher=teacher,\n",
    "    student=student,\n",
    "    projector=projector,\n",
    "    dataloader=train_loader,\n",
    "    device=device,\n",
    "    epochs=1,\n",
    ")\n",
    "\n",
    "# ---------------- phase 2 ----------------\n",
    "# train_head(\n",
    "#     student=student,\n",
    "#     dataloader=train_loader,\n",
    "#     device=device,\n",
    "#     epochs=1,\n",
    "# )\n",
    "\n",
    "\n",
    "# # save student\n",
    "# student.eval()\n",
    "# torch.save(student.state_dict(), \"./checkpoints/vmamba_distilled.pth\")\n",
    "# print(\"saved ./checkpoints/vmamba_distilled.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9e9f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 413 images for split=train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                                path  label\n",
       " 0  ./data/aaryapatel98/indian-diabetic-retinopath...      3\n",
       " 1  ./data/aaryapatel98/indian-diabetic-retinopath...      3\n",
       " 2  ./data/aaryapatel98/indian-diabetic-retinopath...      2\n",
       " 3  ./data/aaryapatel98/indian-diabetic-retinopath...      3\n",
       " 4  ./data/aaryapatel98/indian-diabetic-retinopath...      4,\n",
       " label\n",
       " 2    136\n",
       " 0    134\n",
       " 3     74\n",
       " 4     49\n",
       " 1     20\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from config.constants import IDRID_PATH\n",
    "from dataloader.idrid import IDRiDDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- CONFIG ---\n",
    "\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# --- Load dataset ---\n",
    "ds = IDRiDDataset(IDRID_PATH, split=\"train\", transform=tfm)\n",
    "\n",
    "# --- Build DataFrame ---\n",
    "records = []\n",
    "for p in ds.img_paths:\n",
    "    img_id = p.split(\"/\")[-1].replace(\".jpg\", \"\")\n",
    "    label = ds.grade_map[img_id]\n",
    "    records.append({\"path\": p, \"label\": label})\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "df.head(), df.label.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d27580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 413 images for split=train\n",
      "Image shape: torch.Size([1, 3, 224, 224])\n",
      "\n",
      "--- VISUAL MAMBA MASKING TEST ---\n",
      "Patch seq shape: torch.Size([1, 196, 192])\n",
      "Mask shape: torch.Size([1, 196])\n",
      "ids_keep shape: torch.Size([1, 79])\n",
      "ids_restore shape: torch.Size([1, 196])\n",
      "\n",
      "Total tokens: 196\n",
      "Visible tokens: 79.0\n",
      "Masked tokens: 117.0\n",
      "Mask ratio observed: 0.60 (target: 0.60)\n",
      "\n",
      "Reconstructed seq shape: torch.Size([1, 196, 192])\n",
      "Pooled masked feature shape: torch.Size([1, 192])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from config.constants import *\n",
    "from dataloader.idrid import IDRiDDataset\n",
    "from models.vmamba_backbone import VisualMamba\n",
    "import pandas as pd\n",
    "\n",
    "# ------------------------\n",
    "# Set device\n",
    "# ------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ------------------------\n",
    "# 1. Load Dataset\n",
    "# ------------------------\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "ds = IDRiDDataset(IDRID_PATH, split=\"train\", transform=tfm)\n",
    "x, y = ds[0]    # one sample\n",
    "x = x.unsqueeze(0).to(device)   # (1, 3, 224, 224)\n",
    "\n",
    "print(\"Image shape:\", x.shape)\n",
    "\n",
    "# ------------------------\n",
    "# 2. Create VisualMamba\n",
    "# ------------------------\n",
    "model = VisualMamba(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_chans=IN_CHANS,\n",
    "    embed_dim=VMAMBA_EMBED_DIM,\n",
    "    depth=VMAMBA_DEPTH,\n",
    "    learning_rate=0.0,\n",
    "    mask_ratio=0.6,\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ------------------------\n",
    "# 3. Forward with masking\n",
    "# ------------------------\n",
    "seq, mask, ids_keep, ids_restore = model.forward_features(\n",
    "    x, \n",
    "    return_pooled=False,\n",
    "    apply_mask=True\n",
    ")\n",
    "print(\"\\n--- VISUAL MAMBA MASKING TEST ---\")\n",
    "print(\"Patch seq shape:\", seq.shape)\n",
    "print(\"Mask shape:\", mask.shape)\n",
    "print(\"ids_keep shape:\", ids_keep.shape)\n",
    "print(\"ids_restore shape:\", ids_restore.shape)\n",
    "\n",
    "# ------------------------\n",
    "# 4. Verify Mask Ratio\n",
    "# ------------------------\n",
    "B, N = mask.shape\n",
    "num_visible = mask.sum().item()\n",
    "num_masked = N - num_visible\n",
    "print(f\"\\nTotal tokens: {N}\")\n",
    "print(f\"Visible tokens: {num_visible}\")\n",
    "print(f\"Masked tokens: {num_masked}\")\n",
    "print(f\"Mask ratio observed: {num_masked/N:.2f} (target: 0.60)\")\n",
    "\n",
    "# ------------------------\n",
    "# 5. Check ordering correctness\n",
    "# ------------------------\n",
    "seq_reconstructed = torch.gather(\n",
    "    seq,\n",
    "    dim=1,\n",
    "    index=ids_restore.unsqueeze(-1).expand(-1, -1, model.embed_dim)\n",
    ")\n",
    "\n",
    "print(\"\\nReconstructed seq shape:\", seq_reconstructed.shape)\n",
    "\n",
    "# ------------------------\n",
    "# 6. Check pooled output correctness\n",
    "# ------------------------\n",
    "pooled = model.forward_features(\n",
    "    x,\n",
    "    return_pooled=True,\n",
    "    apply_mask=True\n",
    ")\n",
    "\n",
    "print(\"Pooled masked feature shape:\", pooled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "620a025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 413 images for split=train\n",
      "Found 103 images for split=test\n",
      "Loading checkpoint: vmamba_full_supervised/gg6er7wp/checkpoints/vmamba_supervised_best.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['class_weights', 'loss_fn.weight']\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions...\n",
      "Found 413 images for split=train\n",
      "Found 103 images for split=test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df4db36b0314ac5ab4b073ebfbc789f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Saved predictions to vmamba_idrid_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# predict_vmamba_idrid.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "from config.constants import *\n",
    "from utils.vmamba_idrid import VmambaClassifier\n",
    "from dataloader.idrid import IDRiDModule\n",
    "\n",
    "\n",
    "MODEL_PATH = \"vmamba_full_supervised/gg6er7wp/checkpoints/vmamba_supervised_best.ckpt\"\n",
    "CSV_OUT = \"vmamba_idrid_predictions.csv\"\n",
    "\n",
    "\n",
    "def run_prediction():\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    # Transform ‚Äî MUST match training\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Test datamodule\n",
    "    dm = IDRiDModule(\n",
    "        root=IDRID_PATH,\n",
    "        transform=tfm,\n",
    "        batch_size=1,\n",
    "    )\n",
    "    dm.setup(stage=\"test\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # LOAD MODEL SAFELY\n",
    "    # --------------------------------------------------------\n",
    "    print(f\"Loading checkpoint: {MODEL_PATH}\")\n",
    "\n",
    "    model = VmambaClassifier.load_from_checkpoint(\n",
    "        MODEL_PATH,\n",
    "        lr=1e-4,\n",
    "        class_weights=None,\n",
    "        strict=False\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "        devices=1,\n",
    "    )\n",
    "\n",
    "    print(\"Running predictions...\")\n",
    "    outputs = trainer.predict(model, datamodule=dm)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # FLATTEN PREDICTIONS\n",
    "    # --------------------------------------------------------\n",
    "    paths, labels, preds, all_probs = [], [], [], []\n",
    "\n",
    "    for batch in outputs:\n",
    "        paths.extend(batch[\"paths\"])\n",
    "        labels.extend(batch[\"labels\"].tolist())\n",
    "        preds.extend(batch[\"preds\"].tolist())\n",
    "        all_probs.extend(batch[\"probs\"].tolist())\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # SAVE TO CSV\n",
    "    # --------------------------------------------------------\n",
    "    df = pd.DataFrame({\n",
    "        \"image_path\": paths,\n",
    "        \"true_label\": labels,\n",
    "        \"pred_label\": preds,\n",
    "    })\n",
    "\n",
    "    probs_df = pd.DataFrame(\n",
    "        all_probs,\n",
    "        columns=[f\"prob_{i}\" for i in range(len(all_probs[0]))],\n",
    "    )\n",
    "\n",
    "    df = pd.concat([df, probs_df], axis=1)\n",
    "    df.to_csv(CSV_OUT, index=False)\n",
    "\n",
    "    print(f\"[‚úì] Saved predictions to {CSV_OUT}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d8e911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUNNING IDRiD DIAGNOSTICS ===\n",
      "\n",
      "Found 413 images for split=train\n",
      "Found 103 images for split=test\n",
      "Train set size: 413\n",
      "Val/Test set size: 103\n",
      "\n",
      "Train label distribution: Counter({2: 136, 0: 134, 3: 74, 4: 49, 1: 20})\n",
      "Val/Test label distribution: Counter({0: 34, 2: 32, 3: 19, 4: 13, 1: 5})\n",
      "\n",
      "First 10 validation labels:\n",
      "IDRiD_001.jpg ‚Üí 4\n",
      "IDRiD_002.jpg ‚Üí 4\n",
      "IDRiD_003.jpg ‚Üí 4\n",
      "IDRiD_004.jpg ‚Üí 4\n",
      "IDRiD_005.jpg ‚Üí 4\n",
      "IDRiD_006.jpg ‚Üí 3\n",
      "IDRiD_007.jpg ‚Üí 3\n",
      "IDRiD_008.jpg ‚Üí 2\n",
      "IDRiD_009.jpg ‚Üí 2\n",
      "IDRiD_010.jpg ‚Üí 2\n",
      "\n",
      "Test CSV head:\n",
      "  Image name  Retinopathy grade  Risk of macular edema \n",
      "0  IDRiD_001                  4                       0\n",
      "1  IDRiD_002                  4                       1\n",
      "2  IDRiD_003                  4                       0\n",
      "3  IDRiD_004                  4                       0\n",
      "4  IDRiD_005                  4                       1\n",
      "\n",
      "Unique 'Retinopathy grade' values in Test CSV: [4 3 2 0 1]\n",
      "\n",
      "NaN labels detected in test CSV: 0\n",
      "No NaN labels in validation mapping.\n",
      "\n",
      "=== DIAGNOSTICS COMPLETE ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dataloader.idrid import IDRiDDataset, IDRiDModule\n",
    "from config.constants import IDRID_PATH, IMG_SIZE\n",
    "from torchvision import transforms\n",
    "from collections import Counter\n",
    "\n",
    "print(\"\\n=== RUNNING IDRiD DIAGNOSTICS ===\\n\")\n",
    "\n",
    "# Same transform used in training\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load your datamodule exactly as before\n",
    "dm = IDRiDModule(root=IDRID_PATH, transform=tfm, batch_size=16)\n",
    "dm.setup()\n",
    "\n",
    "train_ds = dm.train\n",
    "val_ds   = dm.val\n",
    "\n",
    "# ------------------------------------\n",
    "# 1. PRINT DATASET SIZES\n",
    "# ------------------------------------\n",
    "print(f\"Train set size: {len(train_ds)}\")\n",
    "print(f\"Val/Test set size: {len(val_ds)}\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 2. CHECK LABEL DISTRIBUTION\n",
    "# ------------------------------------\n",
    "def count_labels(ds):\n",
    "    labels = []\n",
    "    for i in range(len(ds)):\n",
    "        _, y, _ = ds[i]\n",
    "        labels.append(int(y))\n",
    "    return Counter(labels)\n",
    "\n",
    "print(\"\\nTrain label distribution:\", count_labels(train_ds))\n",
    "print(\"Val/Test label distribution:\", count_labels(val_ds))\n",
    "\n",
    "# ------------------------------------\n",
    "# 3. PRINT FIRST 10 VAL LABELS\n",
    "# ------------------------------------\n",
    "print(\"\\nFirst 10 validation labels:\")\n",
    "for i in range(min(10, len(val_ds))):\n",
    "    _, y, p = val_ds[i]\n",
    "    print(os.path.basename(p), \"‚Üí\", y)\n",
    "\n",
    "# ------------------------------------\n",
    "# 4. CHECK IF TEST CSV HAS REAL LABELS\n",
    "# ------------------------------------\n",
    "test_csv = os.path.join(\n",
    "    IDRID_PATH, \"2. Groundtruths\", \"b. IDRiD_Disease Grading_Testing Labels.csv\"\n",
    ")\n",
    "if os.path.exists(test_csv):\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "    print(\"\\nTest CSV head:\")\n",
    "    print(df_test.head())\n",
    "    print(\"\\nUnique 'Retinopathy grade' values in Test CSV:\", df_test[\"Retinopathy grade\"].unique())\n",
    "\n",
    "# ------------------------------------\n",
    "# 5. CHECK IF ANY VAL LABEL IS NaN ‚Üí FORCED TO 0\n",
    "# ------------------------------------\n",
    "nan_count = 0\n",
    "for name, grade in val_ds.grade_map.items():\n",
    "    if pd.isna(grade):\n",
    "        nan_count += 1\n",
    "\n",
    "print(f\"\\nNaN labels detected in test CSV: {nan_count}\")\n",
    "\n",
    "if nan_count > 0:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Test CSV contains NaN labels ‚Üí all NaN become class 0\")\n",
    "else:\n",
    "    print(\"No NaN labels in validation mapping.\")\n",
    "\n",
    "print(\"\\n=== DIAGNOSTICS COMPLETE ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0076ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_step source:\n",
      "\n",
      "    def predict_step(self, batch, batch_idx):\n",
      "        x, y, paths = batch\n",
      "        logits = self(x)\n",
      "        probs = torch.softmax(logits, dim=1)\n",
      "        preds = torch.argmax(probs, dim=1)\n",
      "\n",
      "        return {\n",
      "            \"paths\": paths,\n",
      "            \"labels\": y.cpu(),\n",
      "            \"preds\": preds.cpu(),\n",
      "            \"probs\": probs.cpu(),\n",
      "        }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.vmamba_idrid import VmambaClassifier\n",
    "\n",
    "model = VmambaClassifier.load_from_checkpoint(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(\"predict_step source:\\n\")\n",
    "import inspect\n",
    "print(inspect.getsource(model.predict_step))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244353a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in checkpoint:\n",
      "['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'MixedPrecision', 'hparams_name', 'hyper_parameters']\n",
      "\n",
      "State dict keys:\n",
      "class_weights\n",
      "backbone.mask_token\n",
      "backbone.patch_embed.weight\n",
      "backbone.patch_embed.bias\n",
      "backbone.backbone.0.A_log\n",
      "backbone.backbone.0.D\n",
      "backbone.backbone.0.in_proj.weight\n",
      "backbone.backbone.0.conv1d.weight\n",
      "backbone.backbone.0.conv1d.bias\n",
      "backbone.backbone.0.x_proj.weight\n",
      "backbone.backbone.0.dt_proj.weight\n",
      "backbone.backbone.0.dt_proj.bias\n",
      "backbone.backbone.0.out_proj.weight\n",
      "backbone.backbone.1.A_log\n",
      "backbone.backbone.1.D\n",
      "backbone.backbone.1.in_proj.weight\n",
      "backbone.backbone.1.conv1d.weight\n",
      "backbone.backbone.1.conv1d.bias\n",
      "backbone.backbone.1.x_proj.weight\n",
      "backbone.backbone.1.dt_proj.weight\n",
      "backbone.backbone.1.dt_proj.bias\n",
      "backbone.backbone.1.out_proj.weight\n",
      "backbone.backbone.2.A_log\n",
      "backbone.backbone.2.D\n",
      "backbone.backbone.2.in_proj.weight\n",
      "backbone.backbone.2.conv1d.weight\n",
      "backbone.backbone.2.conv1d.bias\n",
      "backbone.backbone.2.x_proj.weight\n",
      "backbone.backbone.2.dt_proj.weight\n",
      "backbone.backbone.2.dt_proj.bias\n",
      "backbone.backbone.2.out_proj.weight\n",
      "backbone.backbone.3.A_log\n",
      "backbone.backbone.3.D\n",
      "backbone.backbone.3.in_proj.weight\n",
      "backbone.backbone.3.conv1d.weight\n",
      "backbone.backbone.3.conv1d.bias\n",
      "backbone.backbone.3.x_proj.weight\n",
      "backbone.backbone.3.dt_proj.weight\n",
      "backbone.backbone.3.dt_proj.bias\n",
      "backbone.backbone.3.out_proj.weight\n",
      "backbone.backbone.4.A_log\n",
      "backbone.backbone.4.D\n",
      "backbone.backbone.4.in_proj.weight\n",
      "backbone.backbone.4.conv1d.weight\n",
      "backbone.backbone.4.conv1d.bias\n",
      "backbone.backbone.4.x_proj.weight\n",
      "backbone.backbone.4.dt_proj.weight\n",
      "backbone.backbone.4.dt_proj.bias\n",
      "backbone.backbone.4.out_proj.weight\n",
      "backbone.backbone.5.A_log\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ckpt = torch.load(\"vmamba_full_supervised/gg6er7wp/checkpoints/vmamba_supervised_best.ckpt\", map_location=\"cpu\")\n",
    "\n",
    "print(\"Keys in checkpoint:\")\n",
    "print(list(ckpt.keys()))\n",
    "\n",
    "if \"state_dict\" in ckpt:\n",
    "    print(\"\\nState dict keys:\")\n",
    "    for k in list(ckpt[\"state_dict\"].keys())[:50]:\n",
    "        print(k)\n",
    "else:\n",
    "    print(\"\\nRaw top-level keys:\")\n",
    "    for k in list(ckpt.keys())[:50]:\n",
    "        print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bb8c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched weight tensors: 223\n",
      "Total in checkpoint: 225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/code/StudentMAE/.venv/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['class_weights', 'loss_fn.weight']\n"
     ]
    }
   ],
   "source": [
    "model = VmambaClassifier.load_from_checkpoint(MODEL_PATH, lr=1e-4, class_weights=None, strict=False)\n",
    "\n",
    "# Count how many params successfully loaded\n",
    "ckpt = torch.load(MODEL_PATH, map_location=\"cpu\")[\"state_dict\"]\n",
    "model_state = model.state_dict()\n",
    "\n",
    "matched = 0\n",
    "for k in ckpt:\n",
    "    if k in model_state and ckpt[k].shape == model_state[k].shape:\n",
    "        matched += 1\n",
    "\n",
    "print(\"Matched weight tensors:\", matched)\n",
    "print(\"Total in checkpoint:\", len(ckpt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8d7f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RUNNING MODEL + DATALOADER DIAGNOSTICS ===\n",
      "\n",
      "Found 103 images for split=test\n",
      "Predict dataset size: 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded batch of 16 images\n",
      "\n",
      "=== SAMPLE OUTPUT ===\n",
      "True labels: [4, 4, 4, 4, 4, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3]\n",
      "Pred labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sample probs: [0.20288433134555817, 0.19833576679229736, 0.19640283286571503, 0.20164941251277924, 0.20072759687900543]\n",
      "\n",
      "All probs identical across batch?: True\n",
      "\n",
      "=== CLASSIFIER HEAD PARAM STATS ===\n",
      "head.weight  | mean: -0.002200536197051406  | std: 0.03709734231233597  | grad: True\n",
      "head.bias  | mean: -0.025173192843794823  | std: 0.020051641389727592  | grad: True\n",
      "\n",
      "Non-zero parameters in model: 6176453\n",
      "\n",
      "Trainable params: 6185669\n",
      "Frozen params:    0\n",
      "\n",
      "Head weight sample: tensor([-0.0545,  0.0025,  0.0117,  0.0068, -0.0585,  0.0149, -0.0204,  0.0328,\n",
      "         0.0521, -0.0157,  0.0304, -0.0163, -0.0148, -0.0668,  0.0422,  0.0294,\n",
      "         0.0666,  0.0270,  0.0706,  0.0269], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "\n",
      "=== DIAGNOSTICS COMPLETE ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader.idrid import IDRiDDataset\n",
    "from config.constants import IDRID_PATH, IMG_SIZE\n",
    "from torchvision import transforms\n",
    "\n",
    "print(\"\\n=== RUNNING MODEL + DATALOADER DIAGNOSTICS ===\\n\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 0. BUILD THE SAME TRANSFORM\n",
    "# ------------------------------------\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# ------------------------------------\n",
    "# 1. BUILD PREDICT DATASET EXACTLY LIKE trainer.predict()\n",
    "# ------------------------------------\n",
    "predict_ds = IDRiDDataset(\n",
    "    root=IDRID_PATH,\n",
    "    split=\"test\",       # or \"val\" depending on your use\n",
    "    transform=tfm\n",
    ")\n",
    "\n",
    "predict_loader = DataLoader(\n",
    "    predict_ds,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "print(f\"Predict dataset size: {len(predict_ds)}\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 2. LOAD ONE BATCH\n",
    "# ------------------------------------\n",
    "batch = next(iter(predict_loader))\n",
    "x, y, paths = batch\n",
    "\n",
    "print(f\"\\nLoaded batch of {x.shape[0]} images\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 3. MOVE BATCH TO MODEL DEVICE\n",
    "# ------------------------------------\n",
    "device = next(model.parameters()).device\n",
    "x = x.to(device)\n",
    "\n",
    "# ------------------------------------\n",
    "# 4. RUN MODEL ON BATCH\n",
    "# ------------------------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "print(\"\\n=== SAMPLE OUTPUT ===\")\n",
    "print(\"True labels:\", y.tolist())\n",
    "print(\"Pred labels:\", preds.cpu().tolist())\n",
    "print(\"Sample probs:\", probs[0].cpu().tolist())\n",
    "\n",
    "# ------------------------------------\n",
    "# 5. CHECK IF ALL PROBS ARE IDENTICAL\n",
    "# ------------------------------------\n",
    "all_same = torch.allclose(\n",
    "    probs.cpu(), \n",
    "    probs[0].cpu().unsqueeze(0).expand_as(probs.cpu())\n",
    ")\n",
    "print(\"\\nAll probs identical across batch?:\", all_same)\n",
    "\n",
    "# ------------------------------------\n",
    "# 6. CHECK CLASSIFIER HEAD PARAMETERS\n",
    "# ------------------------------------\n",
    "print(\"\\n=== CLASSIFIER HEAD PARAM STATS ===\")\n",
    "for name, p in model.named_parameters():\n",
    "    if \"head\" in name.lower() or \"fc\" in name.lower():\n",
    "        print(name, \" | mean:\", float(p.data.mean()),\n",
    "              \" | std:\", float(p.data.std()),\n",
    "              \" | grad:\", p.requires_grad)\n",
    "\n",
    "# ------------------------------------\n",
    "# 7. CHECK CHECKPOINT ACTUALLY LOADED NON-ZERO WEIGHTS\n",
    "# ------------------------------------\n",
    "sd = model.state_dict()\n",
    "total_nonzero = sum((v != 0).sum().item() for v in sd.values())\n",
    "print(\"\\nNon-zero parameters in model:\", total_nonzero)\n",
    "\n",
    "# ------------------------------------\n",
    "# 8. COUNT TRAINABLE VS FROZEN PARAMS\n",
    "# ------------------------------------\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "print(f\"\\nTrainable params: {trainable}\")\n",
    "print(f\"Frozen params:    {frozen}\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 9. SHOW CLASSIFIER WEIGHT SAMPLE\n",
    "# ------------------------------------\n",
    "for name, p in model.named_parameters():\n",
    "    if \"head\" in name.lower():\n",
    "        print(\"\\nHead weight sample:\", p.flatten()[:20])\n",
    "        break\n",
    "\n",
    "print(\"\\n=== DIAGNOSTICS COMPLETE ===\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62dbcf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checkpoint root keys ---\n",
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'MixedPrecision', 'hparams_name', 'hyper_parameters'])\n",
      "\n",
      "--- First 50 keys inside state_dict ---\n",
      "class_weights\n",
      "backbone.mask_token\n",
      "backbone.patch_embed.weight\n",
      "backbone.patch_embed.bias\n",
      "backbone.backbone.0.A_log\n",
      "backbone.backbone.0.D\n",
      "backbone.backbone.0.in_proj.weight\n",
      "backbone.backbone.0.conv1d.weight\n",
      "backbone.backbone.0.conv1d.bias\n",
      "backbone.backbone.0.x_proj.weight\n",
      "backbone.backbone.0.dt_proj.weight\n",
      "backbone.backbone.0.dt_proj.bias\n",
      "backbone.backbone.0.out_proj.weight\n",
      "backbone.backbone.1.A_log\n",
      "backbone.backbone.1.D\n",
      "backbone.backbone.1.in_proj.weight\n",
      "backbone.backbone.1.conv1d.weight\n",
      "backbone.backbone.1.conv1d.bias\n",
      "backbone.backbone.1.x_proj.weight\n",
      "backbone.backbone.1.dt_proj.weight\n",
      "backbone.backbone.1.dt_proj.bias\n",
      "backbone.backbone.1.out_proj.weight\n",
      "backbone.backbone.2.A_log\n",
      "backbone.backbone.2.D\n",
      "backbone.backbone.2.in_proj.weight\n",
      "backbone.backbone.2.conv1d.weight\n",
      "backbone.backbone.2.conv1d.bias\n",
      "backbone.backbone.2.x_proj.weight\n",
      "backbone.backbone.2.dt_proj.weight\n",
      "backbone.backbone.2.dt_proj.bias\n",
      "backbone.backbone.2.out_proj.weight\n",
      "backbone.backbone.3.A_log\n",
      "backbone.backbone.3.D\n",
      "backbone.backbone.3.in_proj.weight\n",
      "backbone.backbone.3.conv1d.weight\n",
      "backbone.backbone.3.conv1d.bias\n",
      "backbone.backbone.3.x_proj.weight\n",
      "backbone.backbone.3.dt_proj.weight\n",
      "backbone.backbone.3.dt_proj.bias\n",
      "backbone.backbone.3.out_proj.weight\n",
      "backbone.backbone.4.A_log\n",
      "backbone.backbone.4.D\n",
      "backbone.backbone.4.in_proj.weight\n",
      "backbone.backbone.4.conv1d.weight\n",
      "backbone.backbone.4.conv1d.bias\n",
      "backbone.backbone.4.x_proj.weight\n",
      "backbone.backbone.4.dt_proj.weight\n",
      "backbone.backbone.4.dt_proj.bias\n",
      "backbone.backbone.4.out_proj.weight\n",
      "backbone.backbone.5.A_log\n",
      "\n",
      "--- First 50 keys in current model state_dict ---\n",
      "backbone.mask_token\n",
      "backbone.patch_embed.weight\n",
      "backbone.patch_embed.bias\n",
      "backbone.backbone.0.A_log\n",
      "backbone.backbone.0.D\n",
      "backbone.backbone.0.in_proj.weight\n",
      "backbone.backbone.0.conv1d.weight\n",
      "backbone.backbone.0.conv1d.bias\n",
      "backbone.backbone.0.x_proj.weight\n",
      "backbone.backbone.0.dt_proj.weight\n",
      "backbone.backbone.0.dt_proj.bias\n",
      "backbone.backbone.0.out_proj.weight\n",
      "backbone.backbone.1.A_log\n",
      "backbone.backbone.1.D\n",
      "backbone.backbone.1.in_proj.weight\n",
      "backbone.backbone.1.conv1d.weight\n",
      "backbone.backbone.1.conv1d.bias\n",
      "backbone.backbone.1.x_proj.weight\n",
      "backbone.backbone.1.dt_proj.weight\n",
      "backbone.backbone.1.dt_proj.bias\n",
      "backbone.backbone.1.out_proj.weight\n",
      "backbone.backbone.2.A_log\n",
      "backbone.backbone.2.D\n",
      "backbone.backbone.2.in_proj.weight\n",
      "backbone.backbone.2.conv1d.weight\n",
      "backbone.backbone.2.conv1d.bias\n",
      "backbone.backbone.2.x_proj.weight\n",
      "backbone.backbone.2.dt_proj.weight\n",
      "backbone.backbone.2.dt_proj.bias\n",
      "backbone.backbone.2.out_proj.weight\n",
      "backbone.backbone.3.A_log\n",
      "backbone.backbone.3.D\n",
      "backbone.backbone.3.in_proj.weight\n",
      "backbone.backbone.3.conv1d.weight\n",
      "backbone.backbone.3.conv1d.bias\n",
      "backbone.backbone.3.x_proj.weight\n",
      "backbone.backbone.3.dt_proj.weight\n",
      "backbone.backbone.3.dt_proj.bias\n",
      "backbone.backbone.3.out_proj.weight\n",
      "backbone.backbone.4.A_log\n",
      "backbone.backbone.4.D\n",
      "backbone.backbone.4.in_proj.weight\n",
      "backbone.backbone.4.conv1d.weight\n",
      "backbone.backbone.4.conv1d.bias\n",
      "backbone.backbone.4.x_proj.weight\n",
      "backbone.backbone.4.dt_proj.weight\n",
      "backbone.backbone.4.dt_proj.bias\n",
      "backbone.backbone.4.out_proj.weight\n",
      "backbone.backbone.5.A_log\n",
      "backbone.backbone.5.D\n"
     ]
    }
   ],
   "source": [
    "# 1) inspect checkpoint keys\n",
    "ckpt = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "print(\"\\n--- Checkpoint root keys ---\")\n",
    "print(ckpt.keys())\n",
    "\n",
    "print(\"\\n--- First 50 keys inside state_dict ---\")\n",
    "for k in list(ckpt[\"state_dict\"].keys())[:50]:\n",
    "    print(k)\n",
    "\n",
    "# 2) compare to your model keys\n",
    "print(\"\\n--- First 50 keys in current model state_dict ---\")\n",
    "for k in list(model.state_dict().keys())[:50]:\n",
    "    print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13c5914a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BACKBONE INTERNAL DIAGNOSTICS ===\n",
      "\n",
      "Found 103 images for split=test\n",
      "Loaded batch size: torch.Size([8, 3, 224, 224])\n",
      "\n",
      "Input pixel means (per image): [0.097906 0.147802 0.184873 0.240009 0.158269 0.223339 0.224439 0.263004]\n",
      "Input pixel stds  (per image): [0.129837 0.193107 0.231126 0.215861 0.160772 0.218617 0.268931 0.258152]\n",
      "\n",
      "Patch-embed shape: torch.Size([8, 192, 14, 14]) -> flattened: torch.Size([8, 196, 192])\n",
      "Patch-embed pooled mean per sample: [0.00121  0.001919 0.002437 0.007141 0.004412 0.005794 0.003175 0.005639]\n",
      "Patch-embed pooled std per sample: [0.072899 0.106935 0.132436 0.141808 0.097445 0.138957 0.157091 0.16712 ]\n",
      "Patch-embed pooled identical across batch?: False\n",
      "\n",
      "Backbone output shape: torch.Size([8, 196, 192])\n",
      "Pooled features mean per sample: [-0.000123 -0.000123 -0.000123 -0.000123 -0.000123 -0.000123 -0.000123\n",
      " -0.000123]\n",
      "Pooled features std  per sample: [0.008024 0.008024 0.008024 0.008024 0.008024 0.008024 0.008024 0.008024]\n",
      "Pooled features identical across batch?: True\n",
      "\n",
      "First backbone block output stats (sample):\n",
      " mean per sample: [-0.00011  -0.000115 -0.000116 -0.000222 -0.000166 -0.000162 -0.000121\n",
      " -0.0002  ]\n",
      " std per sample:  [0.002188 0.003302 0.004043 0.004519 0.003075 0.004267 0.004827 0.005143]\n",
      "First block output identical across batch?: False\n",
      "\n",
      "Parameter stats:\n",
      " patch_embed.weight mean: 6.191106513142586e-05  std: 0.020848501473665237\n",
      " patch_embed.weight any NaN? False\n",
      " patch_embed.bias any NaN? False\n",
      " param backbone.backbone.0.A_log | mean 1.916977 std 0.764157\n",
      "\n",
      "pooled has NaN? False  pooled has Inf? False\n",
      "\n",
      "cosine between pooled feat[0] and feat[1]: 1.0000001192092896\n",
      "\n",
      "=== DIAGNOSTICS DONE ===\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloader.idrid import IDRiDDataset\n",
    "from config.constants import IDRID_PATH, IMG_SIZE\n",
    "from torchvision import transforms\n",
    "\n",
    "print(\"=== BACKBONE INTERNAL DIAGNOSTICS ===\\n\")\n",
    "\n",
    "# build dataset + loader (match what you used)\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "ds = IDRiDDataset(root=IDRID_PATH, split=\"test\", transform=tfm)\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "# get one batch\n",
    "batch = next(iter(loader))\n",
    "x_cpu, y, paths = batch\n",
    "print(\"Loaded batch size:\", x_cpu.shape)\n",
    "\n",
    "# move to model device\n",
    "device = next(model.parameters()).device\n",
    "x = x_cpu.to(device)\n",
    "\n",
    "# 1) input per-image stats (quick sanity)\n",
    "pixel_means = x.view(x.shape[0], -1).mean(dim=1).cpu().tolist()\n",
    "pixel_stds  = x.view(x.shape[0], -1).std(dim=1).cpu().tolist()\n",
    "print(\"\\nInput pixel means (per image):\", np.round(pixel_means, 6))\n",
    "print(\"Input pixel stds  (per image):\", np.round(pixel_stds, 6))\n",
    "\n",
    "# 2) patch_embed conv output stats\n",
    "with torch.no_grad():\n",
    "    pe = model.backbone.patch_embed(x)   # shape (B, D, H', W')\n",
    "    B, D, H_, W_ = pe.shape\n",
    "    pe_flat = pe.flatten(2).transpose(1,2)   # (B, N, D)\n",
    "    pe_pool = pe_flat.mean(dim=1)            # per-sample pooled vector\n",
    "\n",
    "print(\"\\nPatch-embed shape:\", pe.shape, \"-> flattened:\", pe_flat.shape)\n",
    "print(\"Patch-embed pooled mean per sample:\", np.round(pe_pool.mean(dim=1).cpu().numpy(), 6))\n",
    "print(\"Patch-embed pooled std per sample:\", np.round(pe_pool.std(dim=1).cpu().numpy(), 6))\n",
    "\n",
    "# are pe_pool vectors identical?\n",
    "identical_pe = torch.allclose(pe_pool, pe_pool[0].unsqueeze(0).expand_as(pe_pool))\n",
    "print(\"Patch-embed pooled identical across batch?:\", bool(identical_pe))\n",
    "\n",
    "# 3) backbone output BEFORE norm\n",
    "with torch.no_grad():\n",
    "    # forward through backbone modules manually to inspect intermediate\n",
    "    x_seq = pe_flat  # (B, N, D)\n",
    "    # if backbone is nn.Sequential of Mamba blocks:\n",
    "    for i, block in enumerate(model.backbone.backbone if hasattr(model.backbone, \"backbone\") else model.backbone):\n",
    "        x_seq = block(x_seq)\n",
    "        if i == 0:\n",
    "            first_block_out = x_seq.detach().cpu()\n",
    "    # after full backbone, run norm\n",
    "    x_seq_after = model.backbone.norm(x_seq)  # (B, N, D)\n",
    "    pooled = x_seq_after.mean(dim=1)          # final pooled features\n",
    "\n",
    "print(\"\\nBackbone output shape:\", x_seq_after.shape)\n",
    "print(\"Pooled features mean per sample:\", np.round(pooled.mean(dim=1).cpu().numpy(), 6))\n",
    "print(\"Pooled features std  per sample:\", np.round(pooled.std(dim=1).cpu().numpy(), 6))\n",
    "identical_pooled = torch.allclose(pooled, pooled[0].unsqueeze(0).expand_as(pooled))\n",
    "print(\"Pooled features identical across batch?:\", bool(identical_pooled))\n",
    "\n",
    "# 4) Check first-block outputs variance\n",
    "print(\"\\nFirst backbone block output stats (sample):\")\n",
    "print(\" mean per sample:\", np.round(first_block_out.mean(dim=(1,2)).numpy(), 6))\n",
    "print(\" std per sample: \", np.round(first_block_out.std(dim=(1,2)).numpy(), 6))\n",
    "identical_first_block = bool(torch.allclose(first_block_out, first_block_out[0].unsqueeze(0).expand_as(first_block_out)))\n",
    "print(\"First block output identical across batch?:\", identical_first_block)\n",
    "\n",
    "# 5) Parameter sanity for patch_embed and first Mamba block\n",
    "print(\"\\nParameter stats:\")\n",
    "pe_w = model.backbone.patch_embed.weight.detach().cpu()\n",
    "print(\" patch_embed.weight mean:\", float(pe_w.mean()), \" std:\", float(pe_w.std()))\n",
    "print(\" patch_embed.weight any NaN?\", bool(torch.isnan(pe_w).any()))\n",
    "print(\" patch_embed.bias any NaN?\", bool(torch.isnan(model.backbone.patch_embed.bias.detach().cpu()).any()))\n",
    "\n",
    "# try printing a sample param from first block (if exists)\n",
    "first_param_found = False\n",
    "for n, p in model.named_parameters():\n",
    "    if n.startswith(\"backbone.backbone.0\"):\n",
    "        print(f\" param {n} | mean {float(p.data.mean()):.6f} std {float(p.data.std()):.6f}\")\n",
    "        first_param_found = True\n",
    "        break\n",
    "if not first_param_found:\n",
    "    print(\" No param path backbone.backbone.0.* found (structure differs).\")\n",
    "\n",
    "# 6) detect any NaNs/Infs in pooled features\n",
    "has_nan = torch.isnan(pooled).any().item()\n",
    "has_inf = torch.isinf(pooled).any().item()\n",
    "print(\"\\npooled has NaN?\", has_nan, \" pooled has Inf?\", has_inf)\n",
    "\n",
    "# 7) quick compare two different images individually\n",
    "with torch.no_grad():\n",
    "    f0 = pooled[0].cpu().numpy()\n",
    "    f1 = pooled[1].cpu().numpy()\n",
    "    cos = np.dot(f0, f1) / (np.linalg.norm(f0) * np.linalg.norm(f1) + 1e-12)\n",
    "print(\"\\ncosine between pooled feat[0] and feat[1]:\", float(cos))\n",
    "\n",
    "print(\"\\n=== DIAGNOSTICS DONE ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162a57e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
